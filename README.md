# Kaggle Challenge: Contradictory, My Dear Watson

## Introduction
This repository contains our work for the "Contradictory, My Dear Watson" challenge hosted on Kaggle. We successfully placed 4th out of 55 teams by achieving an accuracy of 92%.

## Challenge Description
In this challenge, participants were tasked with developing models capable of identifying contradictions within pairs of sentences across multiple languages. The primary goal was to enhance machine understanding of natural language intricacies in different linguistic contexts, thereby advancing NLP technologies in areas like automated reasoning and text analysis.

(https://www.kaggle.com/competitions/contradictory-my-dear-watson)

## Our Approach
Our methodology involved:
- Utilizing state-of-the-art transformer models like XLM-Roberta for multilingual capabilities.
- Training with a combination of datasets including a provided dataset and the Multi-Genre Natural Language Inference (MultiNLI) corpus.
- Employing techniques like fine-tuning on language-specific nuances and rigorous validation strategies.

## Results
The model demonstrated high proficiency in detecting contradictory statements with a final accuracy of 92%. This performance is indicative of the effectiveness of our model architecture and training approach.

---
